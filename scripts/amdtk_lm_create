#!/usr/bin/env python

"""Create a Bayesian language model from a text."""

import argparse
import pickle
import amdtk
from amdtk.models.hierarchical_language_model import VOCAB_START
from amdtk.models import HierarchicalPitmanYorProcess


def main():
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--niter",  type=int, default=10,
                        help="number of iteration for the sampling")
    parser.add_argument("--resample",  action="store_true",
                        help="Activate hyperparameter resampling. This" +
                             " updates concentration and discount for" +
                             " every level of the hierarchy (default: False).")
    parser.add_argument("--tokenizer", choices=["word", "char", "phone"],
                        default="word")
    parser.add_argument("--export_fst", action="store_true",
                        help="Converts the Language model into an FSA in " +
                             "openFst format and stores it into a file named" +
                             " like the language model with additional ending" +
                             ".fst.txt")
    parser.add_argument("params", help="parameters for each level of the "
                                       "HPYP as column separated string: d0,"
                                       "c0:d1,c1:...")
    parser.add_argument("vocab", help="list of unique word in the corpus")
    parser.add_argument('text', help='text to train the LM on')
    parser.add_argument('out', help='output model')
    parser.add_argument('symbols_file', type=str,
                        default="syms.txt", nargs='?')

    args = parser.parse_args()

    if args.tokenizer == "phone":
        tokenizer = amdtk.phone_tokenize
    elif args.tokenizer == "char":
        tokenizer = amdtk.char_tokenize
    else:
        tokenizer = amdtk.word_tokenize

    # Load the vocabulary.
    vocab = {}
    with open(args.vocab, 'r') as f:
        for i, line in enumerate(f, 1):
            vocab[line.strip()] = VOCAB_START + i

    # Prepare the text and extract the vocabulary.
    with open(args.text, 'r') as f:
        data_text = f.readlines()

    data_int = amdtk.prepareText(data_text, vocab, tokenize=tokenizer)

    # Load the parameters of the HPYP.
    hpyp_params = amdtk.parseLMParams(args.params)

    # Create and initialize the model.
    G0 = 1.0 / len(vocab)
    model = HierarchicalPitmanYorProcess(hpyp_params, G0, vocab)
    amdtk.initNgramLM(model, data_int)

    llh = amdtk.NgramLMLogLikelihood(model, data_int)
    print("initial log-likelihood:", llh)

    # Train the language model.
    for i in range(args.niter):
        amdtk.sampleNgramLM(model, data_int)
        llh = amdtk.NgramLMLogLikelihood(model, data_int)
        print("iteration:", i+1, "log-likelihood:", llh)
        if args.resample:
            model.resampleAllHyperparameters()
            model.printOutHyperparameters()

    with open(args.out, 'wb') as f:
        pickle.dump(model, f)

    with open(args.symbols_file, 'w') as syms_f:
        full_vocab = model.getFullVocab()
        for word, id in full_vocab:
            syms_f.write('{word}\t{id}\n'.format(word=word, id=id))

    if args.export_fst:
        with open(args.out + ".fst.txt", 'w') as fst_f:
            grammar_fsa = model.exportGrammarFSA()
            fst_f.writelines(grammar_fsa)

if __name__ == '__main__':
    main()
else:
    raise ImportError('this script cannot be imported')
